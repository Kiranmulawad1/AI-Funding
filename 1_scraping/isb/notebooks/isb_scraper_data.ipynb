{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e77685",
   "metadata": {},
   "source": [
    "# üßº ISB Scraper (Full Serial + First Entry Fix)\n",
    "Fetches all funding programs including edge entries in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51041b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install playwright beautifulsoup4 pandas\n",
    "# !playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def get_all_links_ordered():\n",
    "    url = \"https://isb.rlp.de/service/foerderung.html\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_selector(\".isb-foerderfinder-list2\")  # ensure list loads\n",
    "\n",
    "        # Scroll manually a few times (force all entries to render)\n",
    "        for _ in range(15):\n",
    "            await page.mouse.wheel(0, 1500)\n",
    "            await page.wait_for_timeout(800)\n",
    "\n",
    "        html = await page.content()\n",
    "        browser.close()\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        anchor_tags = soup.select(\".isb-foerderfinder-list2-title a\")\n",
    "\n",
    "        links = []\n",
    "        for a in anchor_tags:\n",
    "            href = a.get(\"href\")\n",
    "            if href and \"/foerderung/\" in href:\n",
    "                full = \"https://isb.rlp.de\" + href if href.startswith(\"/\") else href\n",
    "                links.append(full)\n",
    "\n",
    "        return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def scrape_funding_page(playwright, url):\n",
    "    data = {\n",
    "        \"name\": None, \"description\": None, \"eligibility\": None,\n",
    "        \"amount\": None, \"procedure\": None, \"contact\": None, \"url\": url\n",
    "    }\n",
    "\n",
    "    browser = await playwright.chromium.launch(headless=True)\n",
    "    page = await browser.new_page()\n",
    "    try:\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_timeout(2000)\n",
    "        html = await page.content()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        name = soup.select_one(\"h1.col-md-10.col-sm-12.col-xs-12\")\n",
    "        if name: data[\"name\"] = name.get_text(strip=True)\n",
    "\n",
    "        desc = soup.select_one(\"div.ce-textpic.ce-center.ce-above div.ce-bodytext\")\n",
    "        if desc: data[\"description\"] = desc.get_text(strip=True)\n",
    "\n",
    "        # Accordion Panels\n",
    "        for panel in soup.select(\"div.panel.panel-default\"):\n",
    "            try:\n",
    "                title = panel.select_one(\"h4.panel-title a\").get_text(strip=True).lower()\n",
    "                content = panel.select_one(\"div.panel-collapse .ce-bodytext\").get_text(strip=True)\n",
    "                if \"wer wird\" in title:\n",
    "                    data[\"eligibility\"] = content\n",
    "                elif \"wie wird\" in title or \"finanziert\" in title:\n",
    "                    data[\"amount\"] = content\n",
    "                elif \"beantrag\" in title or \"antrag\" in title:\n",
    "                    data[\"procedure\"] = content\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Contact info\n",
    "        contacts = []\n",
    "        for div in soup.select(\"div.col-xs-12.col-lg-3 div.isb-contact-item\"):\n",
    "            lines = div.select(\"div.isb-contact__line .isb-contact__value\")\n",
    "            parts = [l.get_text(strip=True) for l in lines]\n",
    "            contacts.append(\" | \".join(parts))\n",
    "        if contacts:\n",
    "            data[\"contact\"] = \" || \".join(contacts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "    await browser.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_full_scraper():\n",
    "    links = await get_all_links_ordered()\n",
    "    print(f\"üîó Total links found: {len(links)}\")\n",
    "\n",
    "    all_data = []\n",
    "    async with async_playwright() as p:\n",
    "        for i, link in enumerate(links):\n",
    "            print(f\"‚è≥ [{i+1}/{len(links)}] Scraping: {link}\")\n",
    "            entry = await scrape_funding_page(p, link)\n",
    "            all_data.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"funding-isb.csv\", index=False)\n",
    "    print(\"‚úÖ Data saved to funding-isb.csv\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run it\n",
    "df = await run_full_scraper()\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
