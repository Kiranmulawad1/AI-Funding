# import os
# import re
# import uuid
# import hashlib
# import asyncio
# import fitz  # PyMuPDF
# import streamlit as st
# from datetime import datetime
# from config import OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV, get_openai_client
# from rag_core import query_funding_data
# from utils import present, program_name
# from docx_generator import generate_funding_draft
# from memory import save_query_to_postgres, get_recent_queries, clear_all_queries
# from gpt_recommender import build_gpt_prompt, extract_sources_from_response
# # Import new comprehensive search module
# from expanded_dynamic_search import get_comprehensive_funding_results, ExpandedFundingSearcher
# from modern_styles import apply_modern_styling, create_modern_header, create_feature_box, create_funding_card
# from clarifying_questions import ClarifyingQuestionsManager

# # ------------------ Setup ------------------
# st.set_page_config(
#     page_title="üéØ AI Grant Finder", 
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

# # Apply modern styling
# apply_modern_styling()
# client = get_openai_client()
# questions_manager = ClarifyingQuestionsManager()

# # ------------------ ENV Check ------------------
# missing_keys = [k for k, v in {
#     "OPENAI_API_KEY": OPENAI_API_KEY,
#     "PINECONE_API_KEY": PINECONE_API_KEY,
#     "PINECONE_ENV": PINECONE_ENV,
# }.items() if not v]

# if missing_keys:
#     st.error(f"Missing environment variables: {', '.join(missing_keys)}")
#     st.stop()

# # ------------------ Session State Init ------------------
# for key in [
#     "chat_history", "last_recommendation", "pdf_summary_query", 
#     "pdf_hash", "pending_query", "search_method", "ask_clarifying_questions",
#     "current_funding_questions", "current_draft_questions", "enhanced_query",
#     "waiting_for_clarification", "last_results", "show_draft_questions"
# ]:
#     if key not in st.session_state:
#         if key == "chat_history":
#             st.session_state[key] = []
#         elif key == "search_method":
#             st.session_state[key] = "üöÄ Comprehensive Search (20+ sources)"
#         elif key == "ask_clarifying_questions":
#             st.session_state[key] = True
#         elif key == "show_draft_questions":
#             st.session_state[key] = False
#         else:
#             st.session_state[key] = None

# # ------------------ Sidebar Configuration ------------------
# st.sidebar.title("‚öôÔ∏è Settings")

# if "file_uploader_key" not in st.session_state:
#     st.session_state["file_uploader_key"] = "default_uploader"

# # Search method selection
# st.sidebar.markdown("### üîç Search Options")
# search_option = st.sidebar.radio(
#     "Choose search method:",
#     [
#         "üöÄ Comprehensive Search (20+ sources)",
#         "üíæ Database Search (fastest)",
#         "üîÑ Original Dynamic Search (3 sources)"
#     ],
#     index=0,
#     help="Comprehensive search covers EU, federal, regional, and private funding"
# )

# # Store search preference
# st.session_state.search_method = search_option

# # FIXED: Changed question icon from ‚ùì to üß†
# st.session_state.ask_clarifying_questions = st.sidebar.checkbox(
#     "üß† Enable AI Questions", 
#     value=st.session_state.ask_clarifying_questions,
#     help="Allow AI to ask follow-up questions for better results"
# )

# st.sidebar.markdown("---")

# # PDF Upload Section
# st.sidebar.markdown("### üìÑ Company Profile Upload")
# uploaded_pdf = st.sidebar.file_uploader(
#     "Upload PDF Profile (Optional)", 
#     type=["pdf"], 
#     key=st.session_state["file_uploader_key"]
# )

# if uploaded_pdf:
#     pdf_bytes = uploaded_pdf.getvalue()
#     pdf_hash = hashlib.sha256(pdf_bytes).hexdigest()
    
#     if st.session_state.pdf_hash != pdf_hash:
#         st.session_state.pdf_hash = pdf_hash
#         doc = fitz.open(stream=pdf_bytes, filetype="pdf")
#         full_text = "\n".join(page.get_text() for page in doc).strip()[:6000]
        
#         with st.spinner("Processing PDF..."):
#             prompt = f"""Summarize this company profile into 2‚Äì3 lines for funding search.\nFocus on domain, goals, and funding needs.\n---\n{full_text}\n---"""
#             response = client.chat.completions.create(
#                 model="gpt-3.5-turbo",
#                 messages=[{"role": "user", "content": prompt}]
#             )
#             st.session_state.pdf_summary_query = response.choices[0].message.content.strip()
        
#         st.sidebar.success("‚úÖ PDF processed!")
#         st.sidebar.text_area("Extracted Summary:", st.session_state.pdf_summary_query, height=100)

# st.sidebar.markdown("---")

# # Reset and Clear Options
# st.sidebar.markdown("### üîÑ Actions")
# # Only Reset Chat button in sidebar
# if st.sidebar.button("üÜï Reset Chat", type="secondary", use_container_width=True):
#     for key in [
#         "chat_history", "last_recommendation", "pdf_summary_query",
#         "pending_query", "pdf_hash", "enhanced_query", "waiting_for_clarification"
#     ]:
#         st.session_state.pop(key, None)
#     st.session_state.chat_history = []
#     st.session_state["file_uploader_key"] = str(uuid.uuid4())
#     st.rerun()

# # FIXED: Completely removed coverage info from sidebar
# # All coverage-related code is commented out below:

# # # Add source coverage info based on selected search method
# # if "Comprehensive" in st.session_state.get("search_method", ""):
# #     st.sidebar.info("üåç **Coverage**: EU, Federal, 8 States, Private & Industry sources")
# # elif "Dynamic" in st.session_state.get("search_method", ""):
# #     st.sidebar.info("‚ö° **Coverage**: 3 original sources with live data")
# # else:
# #     st.sidebar.info("üíæ **Coverage**: Pre-indexed database (fastest)")

# # # Commented out Current Settings section
# # # st.sidebar.markdown("---")
# # # st.sidebar.markdown("### üìä Current Settings")
# # # search_method_short = st.session_state.get("search_method", "Database").split(" ")[1] if st.session_state.get("search_method") else "Database"
# # # st.sidebar.write(f"üîç Search: {search_method_short}")
# # # st.sidebar.write(f"‚ùì Ask Questions: {'‚úÖ' if st.session_state.ask_clarifying_questions else '‚ùå'}")

# # ------------------ Main Header ------------------
# create_modern_header(
#     "üéØ AI Grant Finder", 
#     "Discover the perfect funding opportunities for your AI and robotics projects"
# )

# # ------------------ Feature Highlights ------------------
# col1, col2, col3 = st.columns(3)
# with col1:
#     create_feature_box(
#         "üîç", 
#         "Comprehensive Search", 
#         "Search across 20+ funding sources: EU, federal, regional, and private funding"
#     )
# with col2:
#     create_feature_box(
#         "ü§ñ", 
#         "AI-Powered Matching", 
#         "Intelligent ranking and filtering based on your specific needs and location"
#     )
# with col3:
#     create_feature_box(
#         "üìù", 
#         "Draft Generation", 
#         "Automated application drafts tailored to specific funding programs"
#     )

# # ------------------ Chat History Display ------------------
# with st.expander("üïí Recent Queries History", expanded=False):
#     recent_queries = get_recent_queries(limit=10)
    
#     if not recent_queries:
#         st.info("No previous queries found.")
#     else:
#         # Clear history button in main area only
#         col1, col2 = st.columns([3, 1])
#         with col2:
#             if st.button("üßπ Clear All History", key="clear_history_main"):
#                 clear_all_queries()
#                 st.success("All history cleared!")
#                 st.rerun()
        
#         st.markdown("---")
        
#         for i, q in enumerate(recent_queries):
#             with st.container():
#                 try:
#                     timestamp = datetime.fromisoformat(str(q['timestamp']))
#                     formatted_time = timestamp.strftime("%B %d, %Y at %H:%M")
#                 except:
#                     formatted_time = str(q['timestamp'])
                
#                 st.markdown(f"**üìÖ {formatted_time}**")
#                 st.markdown(f"**Query:** {q['query'][:150]}{'...' if len(q['query']) > 150 else ''}")
#                 st.markdown(f"**Source:** `{q['source']}` | **Results:** `{q['result_count']}`")
                
#                 with st.expander(f"View Recommendation #{i+1}"):
#                     st.markdown(q['recommendation'])
                
#                 st.markdown("---")

# # ------------------ Main Chat Interface ------------------
# st.markdown("### üí¨ Chat with AI Grant Finder")

# # Handle clarifying questions workflow
# if st.session_state.get("waiting_for_clarification") == "funding":
#     # Show the original query that triggered questions
#     original_query = st.session_state.get("original_query", "")
#     st.info(f"üí≠ **Original query:** {original_query}")
    
#     st.markdown("### ü§î Let me ask a few questions to find better matches:")
    
#     questions = st.session_state.get("current_funding_questions", [])
#     if questions:
#         answers = {}
        
#         # Use columns to make it more compact
#         for i, q_data in enumerate(questions):
#             question = q_data['question']
#             category = q_data['category']
#             options = q_data.get('options', [])
            
#             if options:
#                 # Store the current answer if it exists
#                 current_key = f"clarify_funding_{i}_{category}"
#                 current_value = st.session_state.get(current_key, "Select an option...")
                
#                 answer = st.selectbox(
#                     question,
#                     ["Select an option..."] + options,
#                     index=0 if current_value == "Select an option..." else options.index(current_value) + 1 if current_value in options else 0,
#                     key=current_key
#                 )
#                 if answer != "Select an option...":
#                     answers[category] = answer
#             else:
#                 answer = st.text_input(question, key=f"clarify_funding_{i}_{category}")
#                 if answer.strip():
#                     answers[category] = answer.strip()
        
#         # Show current answers summary
#         if answers:
#             st.success(f"‚úÖ **Your answers:** {', '.join([f'{k}: {v}' for k, v in answers.items()])}")
        
#         col1, col2 = st.columns(2)
#         with col1:
#             if st.button("üîç Search with Details", type="primary", key="search_with_details"):
#                 if answers:
#                     original_query = st.session_state.get("original_query", "")
#                     enhanced_query = questions_manager.process_funding_answers(original_query, answers)
                    
#                     # Clear the waiting state and trigger search
#                     st.session_state.waiting_for_clarification = None
#                     st.session_state.enhanced_query = enhanced_query
#                     st.session_state.enhanced_processed = False
                    
#                     # Add the enhanced query to chat history immediately
#                     with st.chat_message("user"):
#                         st.markdown(enhanced_query)
#                     st.session_state.chat_history.append({"role": "user", "content": enhanced_query})
                    
#                     st.rerun()
#                 else:
#                     st.warning("Please answer at least one question.")
        
#         with col2:
#             if st.button("‚è≠Ô∏è Skip Questions", type="secondary", key="skip_questions"):
#                 # Use original query without enhancement
#                 original_query = st.session_state.get("original_query", "")
                
#                 st.session_state.waiting_for_clarification = None
#                 st.session_state.enhanced_query = original_query
#                 st.session_state.enhanced_processed = False
                
#                 # Add the original query to chat history
#                 with st.chat_message("user"):
#                     st.markdown(original_query)
#                 st.session_state.chat_history.append({"role": "user", "content": original_query})
                
#                 st.rerun()
    
#     # Don't show the rest of the interface while waiting for clarification
#     st.stop()

# elif st.session_state.get("waiting_for_clarification") == "draft":
#     # Handle draft clarifying questions
#     st.markdown("### üìù Let me ask a few questions for a better application draft:")
    
#     questions = st.session_state.get("current_draft_questions", [])
#     funding_program = st.session_state.get("selected_funding_program", {})
    
#     if questions:
#         answers = {}
        
#         for i, q_data in enumerate(questions):
#             question = q_data['question']
#             category = q_data['category']
#             question_type = q_data.get('type', 'text')
            
#             if question_type == 'select' and 'options' in q_data:
#                 answer = st.selectbox(
#                     question,
#                     ["Select an option..."] + q_data['options'],
#                     key=f"clarify_draft_{i}_{category}"
#                 )
#                 if answer != "Select an option...":
#                     answers[category] = answer
#             else:
#                 answer = st.text_area(
#                     question,
#                     height=100,
#                     key=f"clarify_draft_{i}_{category}"
#                 )
#                 if answer.strip():
#                     answers[category] = answer.strip()
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             # FIXED: Enhanced Draft button now generates and downloads immediately
#             if st.button("üìÑ Enhanced Draft", type="primary", key="enhanced_draft_btn"):
#                 if answers:
#                     with st.spinner("üéØ Generating enhanced application draft..."):
#                         try:
#                             # Process answers into enhanced profile
#                             original_query = st.session_state.get("original_query", "")
#                             enhanced_profile = questions_manager.process_draft_answers(
#                                 original_query, funding_program, answers
#                             )
                            
#                             # Base profile with enhancements
#                             profile = {
#                                 "company_name": "Your Company",
#                                 "location": "Germany",
#                                 "industry": "AI/Robotics",
#                                 "goals": "Innovation and research in AI technology",
#                                 "project_idea": original_query,
#                                 "funding_need": "Research and development funding"
#                             }
#                             profile.update(enhanced_profile)
                            
#                             # Generate draft
#                             docx_data = generate_funding_draft(funding_program, profile, client)
                            
#                             # Clear waiting state
#                             st.session_state.waiting_for_clarification = None
                            
#                             st.success("‚úÖ Enhanced draft generated successfully!")
#                             st.download_button(
#                                 label="üìÑ Download Enhanced Draft",
#                                 data=docx_data,
#                                 file_name="enhanced_funding_draft.docx",
#                                 mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
#                                 key="download_enhanced_draft"
#                             )
                            
#                         except Exception as e:
#                             st.error(f"Error generating enhanced draft: {e}")
#                 else:
#                     st.warning("Please answer at least one question for enhanced draft.")
        
#         with col2:
#             # FIXED: Basic Draft button now generates and downloads immediately
#             if st.button("üìù Basic Draft", type="secondary", key="basic_draft_btn"):
#                 with st.spinner("üéØ Generating basic application draft..."):
#                     try:
#                         # Use basic profile
#                         original_query = st.session_state.get("original_query", "")
#                         profile = {
#                             "company_name": "Your Company",
#                             "location": "Germany", 
#                             "industry": "AI/Robotics",
#                             "goals": "Innovation and research in AI technology",
#                             "project_idea": original_query,
#                             "funding_need": "Research and development funding"
#                         }
                        
#                         # Generate draft
#                         docx_data = generate_funding_draft(funding_program, profile, client)
                        
#                         # Clear waiting state
#                         st.session_state.waiting_for_clarification = None
                        
#                         st.success("‚úÖ Basic draft generated successfully!")
#                         st.download_button(
#                             label="üìÑ Download Basic Draft",
#                             data=docx_data,
#                             file_name="basic_funding_draft.docx",
#                             mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
#                             key="download_basic_draft"
#                         )
                        
#                     except Exception as e:
#                         st.error(f"Error generating basic draft: {e}")
    
#     # Don't show the rest of the interface while waiting for clarification
#     st.stop()

# # Display chat history
# for msg in st.session_state.chat_history:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # Main chat input
# user_input = st.chat_input("Describe your company or ask follow-up questions...")

# # Handle new user input or enhanced query
# query_to_process = None
# if user_input:
#     st.session_state.pop("suppress_query", None)
#     query_to_process = user_input
# elif st.session_state.enhanced_query and not st.session_state.get("enhanced_processed"):
#     query_to_process = st.session_state.enhanced_query
#     st.session_state.enhanced_processed = True
# elif st.session_state.pdf_summary_query and not st.session_state.get("pdf_processed"):
#     query_to_process = st.session_state.pdf_summary_query
#     st.session_state.pdf_processed = True

# # Block processing if waiting for clarification or if suppress flag is set
# if st.session_state.get("waiting_for_clarification") or st.session_state.get("suppress_query"):
#     query_to_process = None

# # Process the query
# if query_to_process and not st.session_state.get("waiting_for_clarification"):
#     # Add user message to chat (only if it's a new user input, not enhanced query)
#     if user_input and not st.session_state.enhanced_query:
#         with st.chat_message("user"):
#             st.markdown(query_to_process)
#         st.session_state.chat_history.append({"role": "user", "content": query_to_process})
    
#     # Handle follow-up questions to existing recommendation
#     if st.session_state.last_recommendation and user_input and not st.session_state.enhanced_query:  # Only for new user input, not enhanced queries
#         follow_up_prompt = f"""You are a funding assistant chatbot.
        
# Previous recommendation you gave:
# ---
# {st.session_state.last_recommendation}
# ---
# User follow-up question: "{user_input}"
# Rules:
# - Only use information from the previous recommendation
# - If information wasn't provided, say it wasn't available
# - Don't make up contact info or details
# - Suggest visiting official URLs only if they were listed
# Respond clearly and helpfully:"""
        
#         with st.chat_message("assistant"):
#             response = client.chat.completions.create(
#                 model="gpt-4-turbo",
#                 messages=[{"role": "user", "content": follow_up_prompt}]
#             )
#             answer = response.choices[0].message.content.strip()
#             st.markdown(answer)
#             st.session_state.chat_history.append({"role": "assistant", "content": answer})
    
#     else:
#         # Check if we should ask clarifying questions (only for new user input)
#         if (user_input and not st.session_state.enhanced_query and 
#             st.session_state.ask_clarifying_questions and 
#             questions_manager.should_ask_funding_questions(query_to_process)):
#             st.session_state.original_query = query_to_process
#             st.session_state.current_funding_questions = questions_manager.generate_funding_questions(query_to_process)
#             st.session_state.waiting_for_clarification = "funding"
#             st.rerun()
        
#         # Perform funding search based on selected method
#         with st.spinner("üîç Searching for funding opportunities..."):
#             search_method = st.session_state.get("search_method", "üíæ Database Search (fastest)")
            
#             if "Comprehensive Search" in search_method:
#                 # Use new comprehensive search (20+ sources)
#                 try:
#                     results = asyncio.run(get_comprehensive_funding_results(query_to_process, max_results=12))
#                     search_method_display = "Comprehensive Search (20+ sources)"
#                 except Exception as e:
#                     st.warning(f"Comprehensive search failed: {e}. Using database search.")
#                     results = query_funding_data(query_to_process)
#                     search_method_display = "Database Search (fallback)"
            
#             elif "Dynamic Search" in search_method:
#                 # Use original dynamic search (3 sources) - fallback to database if not available
#                 try:
#                     from dynamic_search import get_dynamic_funding_results
#                     results = asyncio.run(get_dynamic_funding_results(query_to_process, max_results=8))
#                     search_method_display = "Dynamic Search (3 original sources)"
#                 except Exception as e:
#                     st.warning(f"Dynamic search not available: {e}. Using database search.")
#                     results = query_funding_data(query_to_process)
#                     search_method_display = "Database Search (fallback)"
            
#             else:
#                 # Use existing database search (fastest)
#                 results = query_funding_data(query_to_process)
#                 search_method_display = "Database Search"
        
#         if not results:
#             with st.chat_message("assistant"):
#                 st.error("No matching funding programs found. Try a different query or check your search settings.")
#         else:
#             # Generate GPT recommendation
#             with st.chat_message("assistant"):
#                 message_placeholder = st.empty()
#                 full_response = ""
                
#                 prompt = build_gpt_prompt(query_to_process, results)
#                 response = client.chat.completions.create(
#                     model="gpt-4-turbo",
#                     messages=[{"role": "user", "content": prompt}],
#                     stream=True
#                 )
                
#                 for chunk in response:
#                     if chunk.choices and getattr(chunk.choices[0].delta, "content", None):
#                         token = chunk.choices[0].delta.content
#                         full_response += token
#                         message_placeholder.markdown(full_response + "‚ñå")
                
#                 message_placeholder.markdown(full_response)
                
#                 # Add search method info
#                 st.info(f"üîç Results found using: **{search_method_display}**")
                
#                 # Show enhanced query info if it was used
#                 if st.session_state.enhanced_query:
#                     st.success(f"‚úÖ Enhanced search completed!")
            
#             # Save recommendation and results
#             st.session_state.last_recommendation = full_response
#             st.session_state.last_results = results
#             st.session_state.chat_history.append({"role": "assistant", "content": full_response})
            
#             # Save to database
#             sources = extract_sources_from_response(full_response)
#             source = ", ".join(sorted(sources)) or "Unknown"
#             rec_count = len(results)
#             save_query_to_postgres(query_to_process, f"{source} ({search_method_display})", rec_count, full_response)
            
#             # Clear the enhanced query so it doesn't get processed again
#             st.session_state.enhanced_query = None

# # ------------------ Draft Generation Section ------------------
# if st.session_state.last_recommendation:
#     st.markdown("---")
#     st.markdown("### üìù Generate Application Drafts")
    
#     # Parse funding programs from recommendation
#     funding_blocks = re.split(r"\n(?=#+\s*\d+\.)", st.session_state.last_recommendation.strip())
    
#     # Create columns for draft buttons
#     cols = st.columns(min(len(funding_blocks), 3))
    
#     for idx, block in enumerate(funding_blocks):
#         if block.strip():
#             col_idx = idx % 3
#             with cols[col_idx]:
#                 program_name_match = re.search(r"#+\s*\d+\.\s+(.+?)\s*\(", block)
#                 program_name = program_name_match.group(1) if program_name_match else f"Program {idx + 1}"
                
#                 if st.button(f"üìù Draft for {program_name[:20]}...", key=f"draft_{idx}"):
#                     # Extract program metadata
#                     def extract_field(pattern):
#                         match = re.search(pattern, block, re.DOTALL)
#                         return match.group(1).strip() if match else None
                    
#                     metadata = {}
#                     for field, pattern in {
#                         "name": r"#+\s*\d+\.\s+(.+?)\s*\(",
#                         "domain": r"\*\*Domain\*\*:?\s*(.+)",
#                         "eligibility": r"\*\*Eligibility\*\*:?\s*(.+)",
#                         "amount": r"\*\*Amount\*\*:?\s*(.+)",
#                         "deadline": r"\*\*Deadline\*\*:?\s*(.+)",
#                         "location": r"\*\*Location\*\*:?\s*(.+)",
#                         "contact": r"\*\*Contact\*\*:?\s*(.+)",
#                     }.items():
#                         value = extract_field(pattern)
#                         if value and value.lower() not in ["not specified", "information not found"]:
#                             metadata[field] = value
                    
#                     # Check if we should ask clarifying questions for draft
#                     if st.session_state.ask_clarifying_questions:
#                         original_query = st.session_state.get("original_query", query_to_process or "")
#                         if questions_manager.should_ask_draft_questions(metadata, original_query):
#                             st.session_state.selected_funding_program = metadata
#                             st.session_state.current_draft_questions = questions_manager.generate_draft_questions(metadata, original_query)
#                             st.session_state.waiting_for_clarification = "draft"
#                             st.rerun()
                    
#                     # Generate draft directly if no questions needed
#                     with st.spinner("üéØ Generating application draft..."):
#                         profile = {
#                             "company_name": "Your Company",
#                             "location": "Germany",
#                             "industry": "AI/Robotics",
#                             "goals": "Innovation and research in AI technology",
#                             "project_idea": query_to_process or "AI-based project",
#                             "funding_need": "Research and development funding"
#                         }
                        
#                         # Use enhanced profile if available
#                         if st.session_state.get("enhanced_profile"):
#                             profile.update(st.session_state.enhanced_profile)
                        
#                         try:
#                             docx_data = generate_funding_draft(metadata, profile, client)
                            
#                             st.success("‚úÖ Draft generated successfully!")
#                             st.download_button(
#                                 label=f"üìÑ Download {program_name[:15]}... Draft",
#                                 data=docx_data,
#                                 file_name=f"funding_draft_{idx + 1}.docx",
#                                 mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
#                                 key=f"download_{idx}"
#                             )
#                         except Exception as e:
#                             st.error(f"Error generating draft: {e}")

# # ------------------ Footer ------------------
# st.markdown("---")
# st.markdown(
#     """
#     <div style='text-align: center; color: #666; padding: 2rem;'>
#         <p>üéØ <strong>AI Grant Finder</strong> - Powered by OpenAI GPT-4 & Comprehensive Search</p>
#         <p>üí° <em>Now covering 20+ funding sources across EU, Federal, Regional & Private sectors</em></p>
#     </div>
#     """, 
#     unsafe_allow_html=True
# )

import os
import re
import uuid
import hashlib
import asyncio
import fitz  # PyMuPDF
import streamlit as st
from datetime import datetime
from config import OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENV, get_openai_client
from rag_core import query_funding_data
from utils import present, program_name
from docx_generator import generate_funding_draft
from memory import save_query_to_postgres, get_recent_queries, clear_all_queries
from gpt_recommender import build_gpt_prompt, extract_sources_from_response
# Import new comprehensive search module
from expanded_dynamic_search import get_comprehensive_funding_results, ExpandedFundingSearcher
from modern_styles import apply_modern_styling, create_modern_header, create_feature_box, create_funding_card
from clarifying_questions import ClarifyingQuestionsManager

# ------------------ Setup ------------------
st.set_page_config(
    page_title="üéØ AI Grant Finder", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# Apply modern styling
apply_modern_styling()
client = get_openai_client()
questions_manager = ClarifyingQuestionsManager()

# ------------------ ENV Check ------------------
missing_keys = [k for k, v in {
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "PINECONE_API_KEY": PINECONE_API_KEY,
    "PINECONE_ENV": PINECONE_ENV,
}.items() if not v]

if missing_keys:
    st.error(f"Missing environment variables: {', '.join(missing_keys)}")
    st.stop()

# ------------------ Session State Init ------------------
for key in [
    "chat_history", "last_recommendation", "pdf_summary_query", 
    "pdf_hash", "pending_query", "search_method", "ask_clarifying_questions",
    "current_funding_questions", "current_draft_questions", "enhanced_query",
    "waiting_for_clarification", "last_results", "show_draft_questions", 
    "follow_up_responses", "current_follow_up"  # Added current_follow_up
]:
    if key not in st.session_state:
        if key == "chat_history":
            st.session_state[key] = []
        elif key == "search_method":
            st.session_state[key] = "üöÄ Comprehensive Search (20+ sources)"
        elif key == "ask_clarifying_questions":
            st.session_state[key] = True
        elif key == "show_draft_questions":
            st.session_state[key] = False
        elif key == "follow_up_responses":
            st.session_state[key] = []
        else:
            st.session_state[key] = None

# ------------------ Sidebar Configuration ------------------
st.sidebar.title("‚öôÔ∏è Settings")

if "file_uploader_key" not in st.session_state:
    st.session_state["file_uploader_key"] = "default_uploader"

# Search method selection
st.sidebar.markdown("### üîç Search Options")
search_option = st.sidebar.radio(
    "Choose search method:",
    [
        "üöÄ Comprehensive Search (20+ sources)",
        "üíæ Database Search (fastest)", 
        "üìä Original Sources Only (FDB, ISB, NRW)"
    ],
    index=0,
    help="Comprehensive search covers EU, federal, regional, and private funding"
)

# Store search preference
st.session_state.search_method = search_option

# Clarifying questions toggle with correct text
st.session_state.ask_clarifying_questions = st.sidebar.checkbox(
    "üß† Enable Clarifying Questions", 
    value=st.session_state.ask_clarifying_questions,
    help="Allow AI to ask follow-up questions for better results"
)

st.sidebar.markdown("---")

# PDF Upload Section
st.sidebar.markdown("### üìÑ Company Profile Upload")
uploaded_pdf = st.sidebar.file_uploader(
    "Upload PDF Profile (Optional)", 
    type=["pdf"], 
    key=st.session_state["file_uploader_key"]
)

if uploaded_pdf:
    pdf_bytes = uploaded_pdf.getvalue()
    pdf_hash = hashlib.sha256(pdf_bytes).hexdigest()
    
    if st.session_state.pdf_hash != pdf_hash:
        st.session_state.pdf_hash = pdf_hash
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        full_text = "\n".join(page.get_text() for page in doc).strip()[:6000]
        
        with st.spinner("Processing PDF..."):
            prompt = f"""Summarize this company profile into 2‚Äì3 lines for funding search.\nFocus on domain, goals, and funding needs.\n---\n{full_text}\n---"""
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            st.session_state.pdf_summary_query = response.choices[0].message.content.strip()
        
        st.sidebar.success("‚úÖ PDF processed!")
        st.sidebar.text_area("Extracted Summary:", st.session_state.pdf_summary_query, height=100)

st.sidebar.markdown("---")

# Reset and Clear Options
st.sidebar.markdown("### üîÑ Actions")
# Only Reset Chat button in sidebar
if st.sidebar.button("üÜï Reset Chat", type="secondary", use_container_width=True):
    for key in [
        "chat_history", "last_recommendation", "pdf_summary_query",
        "pending_query", "pdf_hash", "enhanced_query", "waiting_for_clarification",
        "show_draft_questions", "follow_up_responses", "current_follow_up"
    ]:
        st.session_state.pop(key, None)
    st.session_state.chat_history = []
    st.session_state.follow_up_responses = []
    st.session_state["file_uploader_key"] = str(uuid.uuid4())
    st.rerun()

# ------------------ Main Header ------------------
create_modern_header(
    "üéØ AI Grant Finder", 
    "Discover the perfect funding opportunities for your AI and robotics projects"
)

# ------------------ Feature Highlights ------------------
col1, col2, col3 = st.columns(3)
with col1:
    create_feature_box(
        "üîç", 
        "Comprehensive Search", 
        "Search across 20+ funding sources: EU, federal, regional, and private funding"
    )
with col2:
    create_feature_box(
        "ü§ñ", 
        "AI-Powered Matching", 
        "Intelligent ranking and filtering based on your specific needs and location"
    )
with col3:
    create_feature_box(
        "üìù", 
        "Draft Generation", 
        "Automated application drafts tailored to specific funding programs"
    )

# ------------------ Chat History Display ------------------
with st.expander("üïí Recent Queries History", expanded=False):
    recent_queries = get_recent_queries(limit=10)
    
    if not recent_queries:
        st.info("No previous queries found.")
    else:
        # Clear history button in main area only
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("üßπ Clear All History", key="clear_history_main"):
                clear_all_queries()
                st.success("All history cleared!")
                st.rerun()
        
        st.markdown("---")
        
        for i, q in enumerate(recent_queries):
            with st.container():
                try:
                    timestamp = datetime.fromisoformat(str(q['timestamp']))
                    formatted_time = timestamp.strftime("%B %d, %Y at %H:%M")
                except:
                    formatted_time = str(q['timestamp'])
                
                st.markdown(f"**üìÖ {formatted_time}**")
                st.markdown(f"**Query:** {q['query'][:150]}{'...' if len(q['query']) > 150 else ''}")
                st.markdown(f"**Source:** `{q['source']}` | **Results:** `{q['result_count']}`")
                
                with st.expander(f"View Recommendation #{i+1}"):
                    st.markdown(q['recommendation'])
                
                st.markdown("---")

# ------------------ Main Chat Interface ------------------
st.markdown("### üí¨ Chat with AI Grant Finder")

# Handle clarifying questions workflow for funding search only
if st.session_state.get("waiting_for_clarification") == "funding":
    # Show the original query that triggered questions
    original_query = st.session_state.get("original_query", "")
    st.info(f"üí≠ **Original query:** {original_query}")
    
    st.markdown("### ü§î Let me ask a few questions to find better matches:")
    
    questions = st.session_state.get("current_funding_questions", [])
    if questions:
        answers = {}
        
        # Use columns to make it more compact
        for i, q_data in enumerate(questions):
            question = q_data['question']
            category = q_data['category']
            options = q_data.get('options', [])
            
            if options:
                # Store the current answer if it exists
                current_key = f"clarify_funding_{i}_{category}"
                current_value = st.session_state.get(current_key, "Select an option...")
                
                answer = st.selectbox(
                    question,
                    ["Select an option..."] + options,
                    index=0 if current_value == "Select an option..." else options.index(current_value) + 1 if current_value in options else 0,
                    key=current_key
                )
                if answer != "Select an option...":
                    answers[category] = answer
            else:
                answer = st.text_input(question, key=f"clarify_funding_{i}_{category}")
                if answer.strip():
                    answers[category] = answer.strip()
        
        # Show current answers summary
        if answers:
            st.success(f"‚úÖ **Your answers:** {', '.join([f'{k}: {v}' for k, v in answers.items()])}")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîç Search with Details", type="primary", key="search_with_details"):
                if answers:
                    original_query = st.session_state.get("original_query", "")
                    enhanced_query = questions_manager.process_funding_answers(original_query, answers)
                    
                    # Clear the waiting state and trigger search
                    st.session_state.waiting_for_clarification = None
                    st.session_state.enhanced_query = enhanced_query
                    st.session_state.enhanced_processed = False
                    
                    # Add the enhanced query to chat history immediately
                    with st.chat_message("user"):
                        st.markdown(enhanced_query)
                    st.session_state.chat_history.append({"role": "user", "content": enhanced_query})
                    
                    st.rerun()
                else:
                    st.warning("Please answer at least one question.")
        
        with col2:
            if st.button("‚è≠Ô∏è Skip Questions", type="secondary", key="skip_questions"):
                # Use original query without enhancement
                original_query = st.session_state.get("original_query", "")
                
                st.session_state.waiting_for_clarification = None
                st.session_state.enhanced_query = original_query
                st.session_state.enhanced_processed = False
                
                # Add the original query to chat history
                with st.chat_message("user"):
                    st.markdown(original_query)
                st.session_state.chat_history.append({"role": "user", "content": original_query})
                
                st.rerun()
    
    # Don't show the rest of the interface while waiting for funding clarification
    st.stop()

# Display chat history (excluding follow-up questions that will be shown at bottom)
for msg in st.session_state.chat_history:
    # Skip follow-up questions that are stored in follow_up_responses
    is_follow_up_question = False
    if msg["role"] == "user" and st.session_state.get("follow_up_responses"):
        for follow_up in st.session_state.follow_up_responses:
            if msg["content"] == follow_up["question"]:
                is_follow_up_question = True
                break
    
    # Also skip current follow-up question
    if (msg["role"] == "user" and st.session_state.get("current_follow_up") and 
        msg["content"] == st.session_state.current_follow_up["question"]):
        is_follow_up_question = True
    
    # Only display if it's not a follow-up question
    if not is_follow_up_question:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# Main chat input
user_input = st.chat_input("Describe your company or ask follow-up questions...")

# Handle new user input or enhanced query
query_to_process = None
if user_input:
    st.session_state.pop("suppress_query", None)
    query_to_process = user_input
elif st.session_state.enhanced_query and not st.session_state.get("enhanced_processed"):
    query_to_process = st.session_state.enhanced_query
    st.session_state.enhanced_processed = True
elif st.session_state.pdf_summary_query and not st.session_state.get("pdf_processed"):
    query_to_process = st.session_state.pdf_summary_query
    st.session_state.pdf_processed = True

# Block processing if waiting for clarification or if suppress flag is set
if st.session_state.get("waiting_for_clarification") or st.session_state.get("suppress_query"):
    query_to_process = None

# Process the query
if query_to_process and not st.session_state.get("waiting_for_clarification"):
    # Check if this is a follow-up question to existing recommendation
    is_follow_up = (st.session_state.last_recommendation and 
                   user_input and 
                   not st.session_state.enhanced_query and
                   len(query_to_process.split()) < 15)
    
    # Add user message to chat ONLY if it's NOT a follow-up (to prevent duplication)
    if user_input and not st.session_state.enhanced_query and not is_follow_up:
        with st.chat_message("user"):
            st.markdown(query_to_process)
        st.session_state.chat_history.append({"role": "user", "content": query_to_process})
    elif user_input and not st.session_state.enhanced_query and is_follow_up:
        # For follow-ups, add to chat history but prepare for streaming
        # This will be displayed later when we stream the response
        with st.chat_message("user"):
            st.markdown(query_to_process)
        st.session_state.chat_history.append({"role": "user", "content": query_to_process})
    elif not user_input:
        # For enhanced queries or PDF queries, add to chat normally
        with st.chat_message("user"):
            st.markdown(query_to_process)
        st.session_state.chat_history.append({"role": "user", "content": query_to_process})
    
    if is_follow_up:
        # üéØ NEW: Store follow-up for streaming instead of immediate processing
        follow_up_prompt = f"""You are a funding assistant chatbot.
        
Previous recommendation you gave:
---
{st.session_state.last_recommendation}
---
User follow-up question: "{query_to_process}"
Rules:
- Only use information from the previous recommendation
- If information wasn't provided, say it wasn't available
- Don't make up contact info or details
- Suggest visiting official URLs only if they were listed
Respond clearly and helpfully:"""
        
        # Store the question and mark for streaming display
        st.session_state.current_follow_up = {
            "question": query_to_process,
            "prompt": follow_up_prompt,
            "streaming": True
        }
        
        # Clear any draft questions that might be showing
        st.session_state.show_draft_questions = False
        
    else:
        # Handle new search query
        # Clear follow-up responses for new search
        st.session_state.follow_up_responses = []
        
        # Check if we should ask clarifying questions (only for new user input)
        if (user_input and not st.session_state.enhanced_query and 
            st.session_state.ask_clarifying_questions and 
            questions_manager.should_ask_funding_questions(query_to_process)):
            st.session_state.original_query = query_to_process
            st.session_state.current_funding_questions = questions_manager.generate_funding_questions(query_to_process)
            st.session_state.waiting_for_clarification = "funding"
            st.rerun()
        
        # Perform funding search based on selected method
        with st.spinner("üîç Searching for funding opportunities..."):
            search_method = st.session_state.get("search_method", "üíæ Database Search (fastest)")
            
            if "Comprehensive Search" in search_method:
                # Use new comprehensive search (20+ sources)
                try:
                    results = asyncio.run(get_comprehensive_funding_results(query_to_process, max_results=12))
                    search_method_display = "Comprehensive Search (20+ sources)"
                except Exception as e:
                    st.warning(f"Comprehensive search failed: {e}. Using database search.")
                    results = query_funding_data(query_to_process)
                    search_method_display = "Database Search (fallback)"
            
            elif "Original Sources Only" in search_method:
                # Use original 3 sources only (FDB, ISB, NRW) - fallback to database if not available
                try:
                    from dynamic_search import get_dynamic_funding_results
                    results = asyncio.run(get_dynamic_funding_results(query_to_process, max_results=8))
                    search_method_display = "Original Sources Only (FDB, ISB, NRW)"
                except Exception as e:
                    st.warning(f"Original sources search not available: {e}. Using database search.")
                    results = query_funding_data(query_to_process)
                    search_method_display = "Database Search (fallback)"
            
            else:
                # Use existing database search (fastest)
                results = query_funding_data(query_to_process)
                search_method_display = "Database Search"
        
        if not results:
            with st.chat_message("assistant"):
                st.error("No matching funding programs found. Try a different query or check your search settings.")
        else:
            # Generate GPT recommendation
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                prompt = build_gpt_prompt(query_to_process, results)
                response = client.chat.completions.create(
                    model="gpt-4-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    stream=True
                )
                
                for chunk in response:
                    if chunk.choices and getattr(chunk.choices[0].delta, "content", None):
                        token = chunk.choices[0].delta.content
                        full_response += token
                        message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                
                # Add search method info
                st.info(f"üîç Results found using: **{search_method_display}**")
                
                # Show enhanced query info if it was used
                if st.session_state.enhanced_query:
                    st.success(f"‚úÖ Enhanced search completed!")
            
            # Save recommendation and results
            st.session_state.last_recommendation = full_response
            st.session_state.last_results = results
            st.session_state.chat_history.append({"role": "assistant", "content": full_response})
            
            # Save to database
            sources = extract_sources_from_response(full_response)
            source = ", ".join(sorted(sources)) or "Unknown"
            rec_count = len(results)
            save_query_to_postgres(query_to_process, f"{source} ({search_method_display})", rec_count, full_response)
            
            # Clear the enhanced query so it doesn't get processed again
            st.session_state.enhanced_query = None

# ------------------ Draft Generation Section ------------------
if st.session_state.last_recommendation:
    st.markdown("---")
    st.markdown("### üìù Generate Application Drafts")
    
    # Parse funding programs from recommendation
    funding_blocks = re.split(r"\n(?=#+\s*\d+\.)", st.session_state.last_recommendation.strip())
    
    # Create columns for draft buttons
    cols = st.columns(min(len(funding_blocks), 3))
    
    for idx, block in enumerate(funding_blocks):
        if block.strip():
            col_idx = idx % 3
            with cols[col_idx]:
                program_name_match = re.search(r"#+\s*\d+\.\s+(.+?)\s*\(", block)
                program_name = program_name_match.group(1) if program_name_match else f"Program {idx + 1}"
                
                if st.button(f"üìù Draft for {program_name[:20]}...", key=f"draft_{idx}"):
                    # Extract program metadata
                    def extract_field(pattern):
                        match = re.search(pattern, block, re.DOTALL)
                        return match.group(1).strip() if match else None
                    
                    metadata = {}
                    for field, pattern in {
                        "name": r"#+\s*\d+\.\s+(.+?)\s*\(",
                        "domain": r"\*\*Domain\*\*:?\s*(.+)",
                        "eligibility": r"\*\*Eligibility\*\*:?\s*(.+)",
                        "amount": r"\*\*Amount\*\*:?\s*(.+)",
                        "deadline": r"\*\*Deadline\*\*:?\s*(.+)",
                        "location": r"\*\*Location\*\*:?\s*(.+)",
                        "contact": r"\*\*Contact\*\*:?\s*(.+)",
                    }.items():
                        value = extract_field(pattern)
                        if value and value.lower() not in ["not specified", "information not found"]:
                            metadata[field] = value
                    
                    # Store the selected program for potential clarifying questions
                    st.session_state.selected_funding_program = metadata
                    st.session_state.selected_program_idx = idx
                    st.session_state.selected_program_name = program_name
                    
                    # Check if we should ask clarifying questions for draft
                    if st.session_state.ask_clarifying_questions:
                        original_query = st.session_state.get("original_query", query_to_process or "")
                        if questions_manager.should_ask_draft_questions(metadata, original_query):
                            st.session_state.current_draft_questions = questions_manager.generate_draft_questions(metadata, original_query)
                            st.session_state.show_draft_questions = True
                            # Don't rerun, just set the flag so questions show below
                        else:
                            # No questions needed, generate directly
                            with st.spinner("üéØ Generating application draft..."):
                                profile = {
                                    "company_name": "Your Company",
                                    "location": "Germany",
                                    "industry": "AI/Robotics",
                                    "goals": "Innovation and research in AI technology",
                                    "project_idea": query_to_process or "AI-based project",
                                    "funding_need": "Research and development funding"
                                }
                                
                                try:
                                    docx_data = generate_funding_draft(metadata, profile, client)
                                    st.success("‚úÖ Draft generated successfully!")
                                    st.download_button(
                                        label=f"üìÑ Download {program_name[:15]}... Draft",
                                        data=docx_data,
                                        file_name=f"funding_draft_{idx + 1}.docx",
                                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                        key=f"download_direct_{idx}"
                                    )
                                except Exception as e:
                                    st.error(f"Error generating draft: {e}")
                    else:
                        # Clarifying questions disabled, generate directly
                        with st.spinner("üéØ Generating application draft..."):
                            profile = {
                                "company_name": "Your Company",
                                "location": "Germany",
                                "industry": "AI/Robotics", 
                                "goals": "Innovation and research in AI technology",
                                "project_idea": query_to_process or "AI-based project",
                                "funding_need": "Research and development funding"
                            }
                            
                            try:
                                docx_data = generate_funding_draft(metadata, profile, client)
                                st.success("‚úÖ Draft generated successfully!")
                                st.download_button(
                                    label=f"üìÑ Download {program_name[:15]}... Draft",
                                    data=docx_data,
                                    file_name=f"funding_draft_{idx + 1}.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                    key=f"download_basic_{idx}"
                                )
                            except Exception as e:
                                st.error(f"Error generating draft: {e}")

    # Show clarifying questions inline if enabled and triggered
    if st.session_state.get("show_draft_questions"):
        st.markdown("---")
        program_name = st.session_state.get("selected_program_name", "the selected program")
        st.markdown(f"### ü§î Let me ask a few questions for a better application draft for **{program_name}**:")
        
        questions = st.session_state.get("current_draft_questions", [])
        funding_program = st.session_state.get("selected_funding_program", {})
        program_idx = st.session_state.get("selected_program_idx", 0)
        
        if questions:
            answers = {}
            
            for i, q_data in enumerate(questions):
                question = q_data['question']
                category = q_data['category']
                question_type = q_data.get('type', 'text')
                
                if question_type == 'select' and 'options' in q_data:
                    answer = st.selectbox(
                        question,
                        ["Select an option..."] + q_data['options'],
                        key=f"clarify_draft_{program_idx}_{i}_{category}"
                    )
                    if answer != "Select an option...":
                        answers[category] = answer
                else:
                    answer = st.text_area(
                        question,
                        height=100,
                        key=f"clarify_draft_{program_idx}_{i}_{category}"
                    )
                    if answer.strip():
                        answers[category] = answer.strip()
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # Enhanced Draft button
                if st.button("üìÑ Enhanced Draft", type="primary", key=f"enhanced_draft_btn_{program_idx}"):
                    if answers:
                        with st.spinner("üéØ Generating enhanced application draft..."):
                            try:
                                # Process answers into enhanced profile
                                original_query = st.session_state.get("original_query", "")
                                enhanced_profile = questions_manager.process_draft_answers(
                                    original_query, funding_program, answers
                                )
                                
                                # Base profile with enhancements
                                profile = {
                                    "company_name": "Your Company",
                                    "location": "Germany",
                                    "industry": "AI/Robotics",
                                    "goals": "Innovation and research in AI technology",
                                    "project_idea": original_query,
                                    "funding_need": "Research and development funding"
                                }
                                profile.update(enhanced_profile)
                                
                                # Generate draft
                                docx_data = generate_funding_draft(funding_program, profile, client)
                                
                                # Clear questions display
                                st.session_state.show_draft_questions = False
                                
                                st.success("‚úÖ Enhanced draft generated successfully!")
                                st.download_button(
                                    label=f"üìÑ Download Enhanced Draft for {program_name}",
                                    data=docx_data,
                                    file_name=f"enhanced_draft_{program_name.replace(' ', '_')}.docx",
                                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                    key=f"download_enhanced_draft_{program_idx}"
                                )
                                
                            except Exception as e:
                                st.error(f"Error generating enhanced draft: {e}")
                    else:
                        st.warning("Please answer at least one question for enhanced draft.")
            
            with col2:
                # Basic Draft button
                if st.button("üìù Basic Draft", type="secondary", key=f"basic_draft_btn_{program_idx}"):
                    with st.spinner("üéØ Generating basic application draft..."):
                        try:
                            # Use basic profile
                            original_query = st.session_state.get("original_query", "")
                            profile = {
                                "company_name": "Your Company",
                                "location": "Germany", 
                                "industry": "AI/Robotics",
                                "goals": "Innovation and research in AI technology",
                                "project_idea": original_query,
                                "funding_need": "Research and development funding"
                            }
                            
                            # Generate draft
                            docx_data = generate_funding_draft(funding_program, profile, client)
                            
                            # Clear questions display
                            st.session_state.show_draft_questions = False
                            
                            st.success("‚úÖ Basic draft generated successfully!")
                            st.download_button(
                                label=f"üìÑ Download Basic Draft for {program_name}",
                                data=docx_data,
                                file_name=f"basic_draft_{program_name.replace(' ', '_')}.docx",
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                key=f"download_basic_draft_{program_idx}"
                            )
                            
                        except Exception as e:
                            st.error(f"Error generating basic draft: {e}")
            
            with col3:
                # Back button to hide questions
                if st.button("‚¨ÖÔ∏è Back to Drafts", type="secondary", key=f"back_to_drafts_{program_idx}"):
                    st.session_state.show_draft_questions = False
                    st.rerun()

# ------------------ üéØ STREAM FOLLOW-UP RESPONSE AFTER DRAFT BUTTONS ------------------
# Stream follow-up responses at the bottom after draft generation section
if st.session_state.get("current_follow_up") and st.session_state.current_follow_up.get("streaming"):
    st.markdown("---")
    current_followup = st.session_state.current_follow_up
    
    # Show the follow-up question first (at bottom)
    with st.chat_message("user"):
        st.markdown(current_followup["question"])
    
    # Stream the follow-up response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": current_followup["prompt"]}],
            stream=True
        )
        
        for chunk in response:
            if chunk.choices and getattr(chunk.choices[0].delta, "content", None):
                token = chunk.choices[0].delta.content
                full_response += token
                message_placeholder.markdown(full_response + "‚ñå")
        
        message_placeholder.markdown(full_response)
    
    # Store the follow-up Q&A for future display (but don't add to main chat history)
    st.session_state.follow_up_responses.append({
        "question": current_followup["question"],
        "answer": full_response
    })
    
    # Clear current follow-up
    st.session_state.current_follow_up = None
    
    # Rerun to show the completed response properly
    st.rerun()

# ------------------ Display Previous Follow-up Q&A After Draft Buttons ------------------
# This achieves the flow: query -> recommendation -> generate buttons -> follow-up Q&A
if st.session_state.get("follow_up_responses") and not st.session_state.get("current_follow_up"):
    st.markdown("---")
    for i, follow_up in enumerate(st.session_state.follow_up_responses):
        with st.chat_message("user"):
            st.markdown(follow_up["question"])
        with st.chat_message("assistant"):
            st.markdown(follow_up["answer"])

# ------------------ Footer ------------------
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666; padding: 2rem;'>
        <p>üéØ <strong>AI Grant Finder</strong> - Powered by OpenAI GPT-4 & Comprehensive Search</p>
        <p>üí° <em>Now covering 20+ funding sources across EU, Federal, Regional & Private sectors</em></p>
        <p>üé¨ <em>‚ú® New: Real-time streaming responses for all interactions</em></p>
    </div>
    """, 
    unsafe_allow_html=True
)